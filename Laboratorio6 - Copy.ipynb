{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYoFDPyCrFuL"
      },
      "source": [
        "### Universidad del Valle de Guatemala<br>Data Science<br>Laboratorio6<br>\n",
        "\n",
        "#### Integrantes:<br>- Christopher García 20541<br>- Andrea Lam 20102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Temg1eTn4cMi"
      },
      "outputs": [],
      "source": [
        "# Se importan librerías\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MWVY0xiALCG"
      },
      "source": [
        "### Se accede a Kaggle para obtener datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmmC0xt0qgfW",
        "outputId": "0b5ef6df-d4e0-4af8-fbe6-d5e264a0a5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (2.0.6)\n",
            "Requirement already satisfied: bleach in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from requests->kaggle) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\andre\\onedrive\\documentos\\github\\uvg_ds_lab6\\.venv\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fTcsfDpRxjpq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Obtener el directorio actual donde se encuentra el archivo de Jupyter Notebook\n",
        "current_directory = os.path.dirname(os.path.abspath('__file__'))\n",
        "\n",
        "# Ruta completa al directorio .kaggle en el directorio actual\n",
        "kaggle_directory = os.path.join(current_directory, \".kaggle\")\n",
        "\n",
        "# Crear el directorio .kaggle si no existe\n",
        "if not os.path.exists(kaggle_directory):\n",
        "    os.mkdir(kaggle_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J6EIqwzrx1pV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        1 file(s) copied.\n"
          ]
        }
      ],
      "source": [
        "!copy .kaggle\\kaggle.json %userprofile%\\.kaggle\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aoHDZ7dWx8hF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'chmod' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb5gO3REsB4u",
        "outputId": "f251c06c-6a9d-4b7c-e3f1-f7d867f415e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "celeba-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aD7dalHvyte0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# Obtener la ubicación actual de trabajo\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "# Definir una ubicación para la extracción\n",
        "extract_path = os.path.join(current_directory, \"celeba-dataset\")\n",
        "\n",
        "# Crear la carpeta de extracción si no existe\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Extraer el archivo ZIP\n",
        "zip_ref = zipfile.ZipFile('celeba-dataset.zip', 'r')\n",
        "zip_ref.extractall(extract_path)\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Redimensiona las imágenes a 128x128 píxeles\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = dset.ImageFolder(\n",
        "    root=\"./celeba-dataset/img_align_celeba\",\n",
        "    transform=transform  # Utiliza la transformación modificada\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparacion de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define las variables\n",
        "ngpu = 1\n",
        "ngf = 64\n",
        "nc = 3\n",
        "nz = 100\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "workers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Especifica la ruta al directorio de datos\n",
        "data_dir = 'celeba-dataset/img_align_celeba/'\n",
        "\n",
        "# Define la transformación para preprocesar las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(128),            # Redimensiona las imágenes a 128x128 píxeles\n",
        "    transforms.CenterCrop(128),        # Recorta las imágenes al centro\n",
        "    transforms.ToTensor(),             # Convierte las imágenes en tensores\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normaliza los valores de los píxeles a [-1, 1]\n",
        "])\n",
        "\n",
        "# Carga el conjunto de datos\n",
        "dataset = dset.ImageFolder(root=\"celeba-dataset/img_align_celeba\", transform=transform)\n",
        "\n",
        "# Crea un DataLoader para facilitar el acceso a los datos en lotes\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
        "\n",
        "# Define el dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementacion de la GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define el generador y el discriminador\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Capa de entrada: Z dimension -> ngf*8 dimension\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Capa 2: ngf*8 dimension -> ngf*4 dimension\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Capa 3: ngf*4 dimension -> ngf*2 dimension\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Capa 4: ngf*2 dimension -> ngf dimension\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Capa de salida: ngf dimension -> nc canales (3 canales en RGB)\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Capa de entrada: 3 canales de color\n",
        "            nn.Conv2d(nc, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            # Capa de salida: un solo canal\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**# Definicion perdida y optimizadores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define el generador y el discriminador\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "\n",
        "# Define la función de pérdida para la GAN\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define los optimizadores para el generador y el discriminador\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento de la GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_labels = torch.ones((batch_size, 1, 1, 1), device=device)\n",
        "fake_labels = torch.zeros((batch_size, 1, 1, 1), device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Using a target size (torch.Size([128, 1, 1, 1])) that is different to the input size (torch.Size([128, 1, 5, 5])) is deprecated. Please ensure they have the same size.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\GitHub\\UVG_DS_Lab6\\Laboratorio6.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Documentos/GitHub/UVG_DS_Lab6/Laboratorio6.ipynb#X62sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Entrena al discriminador con imágenes reales\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Documentos/GitHub/UVG_DS_Lab6/Laboratorio6.ipynb#X62sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output \u001b[39m=\u001b[39m discriminator(real_images\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Documentos/GitHub/UVG_DS_Lab6/Laboratorio6.ipynb#X62sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss_D_real \u001b[39m=\u001b[39m criterion(output, real_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Documentos/GitHub/UVG_DS_Lab6/Laboratorio6.ipynb#X62sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss_D_real\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Documentos/GitHub/UVG_DS_Lab6/Laboratorio6.ipynb#X62sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Genera un lote de imágenes falsas\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\GitHub\\UVG_DS_Lab6\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\GitHub\\UVG_DS_Lab6\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\GitHub\\UVG_DS_Lab6\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
            "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\GitHub\\UVG_DS_Lab6\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3113\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3111\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3112\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3116\u001b[0m     )\n\u001b[0;32m   3118\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3119\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
            "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([128, 1, 1, 1])) that is different to the input size (torch.Size([128, 1, 5, 5])) is deprecated. Please ensure they have the same size."
          ]
        }
      ],
      "source": [
        "# Entrenamiento de la GAN\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        real_images, _ = data  # Obtén un lote de imágenes reales y sus etiquetas (si es necesario)\n",
        "\n",
        "        # Inicializa los gradientes del discriminador y el generador\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Entrena al discriminador con imágenes reales\n",
        "        output = discriminator(real_images.to(device))\n",
        "        loss_D_real = criterion(output, real_labels)\n",
        "        loss_D_real.backward()\n",
        "\n",
        "        # Genera un lote de imágenes falsas\n",
        "        fake_images = generator(torch.randn(batch_size, nz, 1, 1, device=device))\n",
        "\n",
        "        # Entrena al discriminador con imágenes falsas\n",
        "        output = discriminator(fake_images.detach())\n",
        "        loss_D_fake = criterion(output, fake_labels)\n",
        "        loss_D_fake.backward()\n",
        "\n",
        "        # Pérdida total del discriminador\n",
        "        loss_D = loss_D_real + loss_D_fake\n",
        "\n",
        "        # Actualiza los pesos del discriminador\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Inicializa los gradientes del generador\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Entrena al generador para engañar al discriminador\n",
        "        output = discriminator(fake_images)\n",
        "        loss_G = criterion(output, real_labels)\n",
        "        loss_G.backward()\n",
        "\n",
        "        # Actualiza los pesos del generador\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Imprime estadísticas durante el entrenamiento (puedes personalizar esto)\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(dataloader)}] Loss_D: {loss_D.item()}, Loss_G: {loss_G.item()}')\n",
        "\n",
        "        # Guarda imágenes generadas por el generador al final de cada época (opcional)\n",
        "        if (i + 1) == len(dataloader):\n",
        "            with torch.no_grad():\n",
        "                fake = generator(torch.randn(64, nz, 1, 1, device=device)).detach().cpu()\n",
        "            save_image(fake, f\"images/epoch_{epoch}.png\", normalize=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
